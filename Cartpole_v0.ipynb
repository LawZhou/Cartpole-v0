{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole-v0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPB2LujUyCjwaqPttXxqzg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LawZhou/Cartpole-v0/blob/main/Cartpole_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h75DhChCtC6e"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cwArjxJsb9v"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb3nDj6qtbzV"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF2NlaXJ-JAL",
        "outputId": "bda32746-4c1d-417d-8d56-35c308aea1b5"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f659454ea50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hMCiEvw-w9d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVzpmEifJwis"
      },
      "source": [
        "class CartpoleAgent():\n",
        "    def __init__(self, env, num_episodes, bins=(10, 10, 20, 20), min_lr=0.1, epsilon=0.2, lr=1.0,\n",
        "                 discount_factor=1.0, lr_decay=0.25):\n",
        "        '''\n",
        "        Information about Cartpole-v0 env\n",
        "        check https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py for more details.\n",
        "        Observation:\n",
        "        Type: Box(4)\n",
        "        Num     Observation               Min                     Max\n",
        "        0       Cart Position             -4.8                    4.8\n",
        "        1       Cart Velocity             -Inf                    Inf\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\n",
        "        Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num   Action\n",
        "        0     Push cart to the left\n",
        "        1     Push cart to the right\n",
        "\n",
        "        env: the environment.\n",
        "        num_episodes: number of episodes to train.\n",
        "        bins: a tuple specifies the number of bins for each observation.\n",
        "        min_lr: the minimum learning rate.\n",
        "        epsilon: the probability of exploration.\n",
        "        lr: learning rate.\n",
        "        discount_factor: discount factor.\n",
        "        lr_decay: the rate of learning rate decay.\n",
        "        render: to toggle render during training.\n",
        "        '''\n",
        "        self.num_episodes = num_episodes\n",
        "        self.min_lr = min_lr\n",
        "        self.epsilon = epsilon\n",
        "        self.discount_factor = discount_factor\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr = lr\n",
        "        self.env = env\n",
        "\n",
        "        # Discretize the continuous space using bins.\n",
        "        self.bins = bins\n",
        "        self.position_bins = np.linspace(self.env.observation_space.low[0],\n",
        "                                         self.env.observation_space.high[0], num=self.bins[0])\n",
        "        self.pos_velocity_bins = np.linspace(-4, 4, num=self.bins[1])\n",
        "        self.angle_bins = np.linspace(self.env.observation_space.low[2],\n",
        "                                      self.env.observation_space.high[2], num=self.bins[2])\n",
        "        self.angle_velopcity_bins = np.linspace(-4, 4, num=self.bins[3])\n",
        "\n",
        "        self.Q = np.zeros(self.bins + (self.env.action_space.n,))  # Q-table\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        Train the model for self.num_episodes episodes.\n",
        "        '''\n",
        "        for ep in tqdm(range(self.num_episodes)):\n",
        "            state = self.env.reset()\n",
        "            state = self.discretize_state(state)\n",
        "            self.lr = self.get_learning_rate()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = self.discretize_state(next_state)\n",
        "                self.update_Q(state, action, reward, next_state)\n",
        "                state = next_state\n",
        "\n",
        "    def update_Q(self, state, action, reward, next_state):\n",
        "        '''\n",
        "        Update the Q table by equation:\n",
        "        Q(S, A) <- Q(S, A) + alpha*[reward + discount_factor*max_a(Q(S', a) - Q(S, A))]\n",
        "        '''\n",
        "        self.Q[state][action] += self.lr * (\n",
        "                reward + self.discount_factor * np.max(self.Q[next_state]) - self.Q[state][action])\n",
        "\n",
        "    def discretize_state(self, obs):\n",
        "        '''\n",
        "        Discretize the continuous state using bins.\n",
        "        '''\n",
        "        discrete_pos = np.digitize(obs[0], bins=self.position_bins)-1  # -1 turns bin into index\n",
        "        discrete_pos_vel = np.digitize(obs[1], bins=self.pos_velocity_bins)-1\n",
        "        discrete_angle = np.digitize(obs[2], bins=self.angle_bins)-1\n",
        "        discrete_angle_vel = np.digitize(obs[3], bins=self.angle_velopcity_bins)-1\n",
        "        discrete_state = np.array([discrete_pos, discrete_pos_vel, discrete_angle, discrete_angle_vel]).astype(np.int)\n",
        "        return tuple(discrete_state)\n",
        "\n",
        "    def choose_action(self, state, greedy=False):\n",
        "        '''\n",
        "        Choose action by following epsilon-greedy policy.\n",
        "        '''\n",
        "        if not greedy:\n",
        "            # For training\n",
        "            if np.random.random() < self.epsilon:  # Exploration\n",
        "                return self.env.action_space.sample()\n",
        "            else:\n",
        "                return np.argmax(self.Q[state]) # Exploitation\n",
        "        else:\n",
        "            # For evaluation\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        '''\n",
        "        Decay the learning rate to slow down learning in the later episodes.\n",
        "        '''\n",
        "        return max(self.min_lr, self.lr - self.lr * self.lr_decay)\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        '''\n",
        "        Run an episode using the updated Q table.\n",
        "        '''\n",
        "        state = self.env.reset()\n",
        "        for ep in range(50000):\n",
        "            state = self.discretize_state(state)\n",
        "            action = self.choose_action(state, greedy=True)\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "            state = obs\n",
        "        return ep\n",
        "    \n",
        "    def run_with_render(self):\n",
        "      state = self.discretize_state(self.env.reset())\n",
        "      for i in range(50000):\n",
        "        state = self.discretize_state(state)\n",
        "        action = self.choose_action(state, greedy=True)\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        show_state(self.env, step=i)\n",
        "        if done:\n",
        "          break\n",
        "        state = obs\n",
        "      self.env.close()\n",
        "      print(\"Steps that were run:\",i)\n",
        "\n",
        "def run_episodes(agent, play_eps=2000):\n",
        "    '''\n",
        "    Run {play_eps} episodes and compute average returns.\n",
        "    return True if the problem is solved.\n",
        "    '''\n",
        "    steps_recorder = []\n",
        "    num_solved_ep = 0\n",
        "    solved = False\n",
        "    for _ in range(play_eps):\n",
        "        returns = agent.run()\n",
        "        num_solved_ep += 1 if returns >= 195 else 0\n",
        "        if num_solved_ep >= 100: solved = True\n",
        "        steps_recorder.append(returns)\n",
        "    steps_recorder = np.array(steps_recorder)\n",
        "    avg_rtn = np.mean(steps_recorder)\n",
        "    print(f'Finish with mean steps: {avg_rtn} in {play_eps} episodes')\n",
        "    print(f'{np.count_nonzero(steps_recorder >= 195)} episodes last more than 195 steps.')\n",
        "    if solved:\n",
        "      print('Problem solved.')\n",
        "    return solved, avg_rtn\n",
        "\n",
        "\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    '''\n",
        "    Render the environment.\n",
        "    Reference: https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server\n",
        "    '''\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"Step: %d %s\" % (step, info))\n",
        "    plt.axis('off')\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsiFFdwuJ4QE",
        "outputId": "41cdb7d9-8b89-4094-8a1d-42e48b21417b"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "print('train using Q learning:')\n",
        "env.reset()\n",
        "agent = CartpoleAgent(env, num_episodes=5000)\n",
        "agent.train()\n",
        "solved, avg_rtn = run_episodes(agent)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 107/5000 [00:00<00:04, 1058.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using Q learning:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:58<00:00, 85.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: 176.4495 in 2000 episodes\n",
            "253 episodes last more than 195 steps.\n",
            "Problem solved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "0VdiVjudy4qW",
        "outputId": "72870531-d8c1-4371-a7d0-b8ba97544308"
      },
      "source": [
        "agent.run_with_render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Steps that were run: 176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}