{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole-v0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVUeaCkwtpbMLL5J4jVuRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LawZhou/Cartpole-v0/blob/main/Cartpole_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVzpmEifJwis"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "\r\n",
        "class CartpoleAgent():\r\n",
        "    def __init__(self, env, num_episodes, bins=(10, 10, 20, 20), min_lr=0.1, epsilon=0.2, lr=1.0,\r\n",
        "                 discount_factor=1.0, lr_decay=0.25, render=False):\r\n",
        "        '''\r\n",
        "        Information about Cartpole-v0 env\r\n",
        "        check https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py for more details.\r\n",
        "        Observation:\r\n",
        "        Type: Box(4)\r\n",
        "        Num     Observation               Min                     Max\r\n",
        "        0       Cart Position             -4.8                    4.8\r\n",
        "        1       Cart Velocity             -Inf                    Inf\r\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\r\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\r\n",
        "        Actions:\r\n",
        "        Type: Discrete(2)\r\n",
        "        Num   Action\r\n",
        "        0     Push cart to the left\r\n",
        "        1     Push cart to the right\r\n",
        "\r\n",
        "        env: the environment.\r\n",
        "        num_episodes: number of episodes to train.\r\n",
        "        bins: a tuple specifies the number of bins for each observation.\r\n",
        "        min_lr: the minimum learning rate.\r\n",
        "        epsilon: the probability of exploration.\r\n",
        "        lr: learning rate.\r\n",
        "        discount_factor: discount factor.\r\n",
        "        lr_decay: the rate of learning rate decay.\r\n",
        "        render: to toggle render during training.\r\n",
        "        '''\r\n",
        "        self.num_episodes = num_episodes\r\n",
        "        self.min_lr = min_lr\r\n",
        "        self.epsilon = epsilon\r\n",
        "        self.discount_factor = discount_factor\r\n",
        "        self.lr_decay = lr_decay\r\n",
        "        self.lr = lr\r\n",
        "        self.env = env\r\n",
        "        self.render = render\r\n",
        "        self.render_every = 2000  # Render an episode for every {self.render_every} episodes\r\n",
        "\r\n",
        "        # Discretize the continuous space using bins.\r\n",
        "        self.bins = bins\r\n",
        "        self.position_bins = np.linspace(self.env.observation_space.low[0],\r\n",
        "                                         self.env.observation_space.high[0], num=self.bins[0])\r\n",
        "        self.pos_velocity_bins = np.linspace(-4, 4, num=self.bins[1])\r\n",
        "        self.angle_bins = np.linspace(self.env.observation_space.low[2],\r\n",
        "                                      self.env.observation_space.high[2], num=self.bins[2])\r\n",
        "        self.angle_velopcity_bins = np.linspace(-4, 4, num=self.bins[3])\r\n",
        "\r\n",
        "        self.Q = np.zeros(self.bins + (self.env.action_space.n,))  # Q-table\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "        '''\r\n",
        "        Train the model for self.num_episodes episodes.\r\n",
        "        '''\r\n",
        "        for ep in tqdm(range(self.num_episodes)):\r\n",
        "            state = self.env.reset()\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            self.lr = self.get_learning_rate()\r\n",
        "            done = False\r\n",
        "            while not done:\r\n",
        "                if self.render and ep % 2000 == 0:\r\n",
        "                    env.render()\r\n",
        "                action = self.choose_action(state)\r\n",
        "                next_state, reward, done, _ = self.env.step(action)\r\n",
        "                next_state = self.discretize_state(next_state)\r\n",
        "                self.update_Q(state, action, reward, next_state)\r\n",
        "                state = next_state\r\n",
        "\r\n",
        "    def update_Q(self, state, action, reward, next_state):\r\n",
        "        '''\r\n",
        "        Update the Q table by equation:\r\n",
        "        Q(S, A) <- Q(S, A) + alpha*[reward + discount_factor*max_a(Q(S', a) - Q(S, A))]\r\n",
        "        '''\r\n",
        "        self.Q[state][action] += self.lr * (\r\n",
        "                reward + self.discount_factor * np.max(self.Q[next_state]) - self.Q[state][action])\r\n",
        "\r\n",
        "    def discretize_state(self, obs):\r\n",
        "        '''\r\n",
        "        Discretize the continuous state using bins.\r\n",
        "        '''\r\n",
        "        discrete_pos = np.digitize(obs[0], bins=self.position_bins)-1  # -1 turns bin into index\r\n",
        "        discrete_pos_vel = np.digitize(obs[1], bins=self.pos_velocity_bins)-1\r\n",
        "        discrete_angle = np.digitize(obs[2], bins=self.angle_bins)-1\r\n",
        "        discrete_angle_vel = np.digitize(obs[3], bins=self.angle_velopcity_bins)-1\r\n",
        "        discrete_state = np.array([discrete_pos, discrete_pos_vel, discrete_angle, discrete_angle_vel]).astype(np.int)\r\n",
        "        return tuple(discrete_state)\r\n",
        "\r\n",
        "    def choose_action(self, state, greedy=False):\r\n",
        "        '''\r\n",
        "        Choose action by following epsilon-greedy policy.\r\n",
        "        '''\r\n",
        "        if not greedy:\r\n",
        "            # For training\r\n",
        "            if np.random.random() < self.epsilon:  # Exploration\r\n",
        "                return self.env.action_space.sample()\r\n",
        "            else:\r\n",
        "                return np.argmax(self.Q[state]) # Exploitation\r\n",
        "        else:\r\n",
        "            # For evaluation\r\n",
        "            return np.argmax(self.Q[state])\r\n",
        "\r\n",
        "\r\n",
        "    def get_learning_rate(self):\r\n",
        "        '''\r\n",
        "        Decay the learning rate to slow down learning in the later episodes.\r\n",
        "        '''\r\n",
        "        return max(self.min_lr, self.lr - self.lr * self.lr_decay)\r\n",
        "\r\n",
        "\r\n",
        "    def run(self):\r\n",
        "        '''\r\n",
        "        Run an episode using the updated Q table.\r\n",
        "        '''\r\n",
        "        state = self.env.reset()\r\n",
        "        for ep in range(50000):\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            action = self.choose_action(state, greedy=True)\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "            state = obs\r\n",
        "        return ep\r\n",
        "\r\n",
        "def run_episodes(agent, play_eps=2000):\r\n",
        "    '''\r\n",
        "    Run {play_eps} episodes and compute average returns.\r\n",
        "    return True if the problem is solved.\r\n",
        "    '''\r\n",
        "    stepsRecorder = []\r\n",
        "    num_solved_ep = 0\r\n",
        "    solved = False\r\n",
        "    for _ in range(play_eps):\r\n",
        "        returns = agent.run()\r\n",
        "        num_solved_ep += 1 if returns >= 195 else 0\r\n",
        "        if num_solved_ep >= 100: solved = True\r\n",
        "        stepsRecorder.append(returns)\r\n",
        "    stepsRecorder = np.array(stepsRecorder)\r\n",
        "    print(f'Finish with mean steps: {np.mean(stepsRecorder)} in {play_eps} episodes')\r\n",
        "    print(f'{np.count_nonzero(stepsRecorder >= 195)} episodes last more than 195 steps.')\r\n",
        "    if solved:\r\n",
        "      print('Problem solved.')\r\n",
        "    return solved\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsiFFdwuJ4QE",
        "outputId": "875afdb9-a821-4035-ea17-7298a9e33a3e"
      },
      "source": [
        "env = gym.make('CartPole-v0')\r\n",
        "\r\n",
        "print('train using Q learning:')\r\n",
        "env.reset()\r\n",
        "agent = CartpoleAgent(env, num_episodes=5000, render=False)  # Toggle render to enable render during training\r\n",
        "agent.train()\r\n",
        "solved = run_episodes(agent)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 56/5000 [00:00<00:08, 553.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using Q learning:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:57<00:00, 86.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: 198.8965 in 2000 episodes\n",
            "1983 episodes last more than 195 steps.\n",
            "Problem solved.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}